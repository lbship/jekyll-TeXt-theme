# 一、明确需求

分析数据分析岗位的招聘情况，包括地区分布、薪资水平、职位要求等，了解最新数据分析岗位的情况。

环境：python 3.6

设计的工具：Tableau工具（主要是生成图方便，用matplotlib也可以达到同样的效果）
# 二、数据采集
首先编写爬虫，这里主要是爬取前程无忧和拉勾网，直接上前程无忧的代码：

关于前程无忧爬虫代码，网上有很多教程，不过大部分只取了地区、职位、工资和日期这几个字段，
没有涉及到岗位要求和岗位职责，因为要了解职位的需求以及后面方便画词云，我就自己写了一个代码。
说一下拉勾和前程无忧两者的区别，前程无忧爬了2000页，不过大概只有前24页是跟数据分析有关的岗位，
拉勾网的数据量比较少，全国主要城市爬下来，一共也才2000多条，而且基本集中在北京上海杭州。
调整一下前程无忧爬虫格式跟拉勾一样，把两个表格合并起来一起分析。  
```
# -*- coding:utf-8 -*-  
import urllib  
import re,codecs  
import time,random  
import requests  
from lxml import html  
from urllib import parse  
  
key='数据分析'  
key=parse.quote(parse.quote(key))  
headers={'Host':'search.51job.com',  
        'Upgrade-Insecure-Requests':'1',  
        'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}  
def get_links(page):  
    url ='http://search.51job.com/list/000000,000000,0000,00,9,99,'+key+',2,'+ str(page)+'.html'  
    r= requests.get(url,headers,timeout=10)  
    s=requests.session()  
    s.keep_alive = False  
    r.encoding = 'gbk'  
    reg = re.compile(r'class="t1 ">.*? <a target="_blank" title=".*?" href="(.*?)".*? <span class="t2">', re.S)  
    links = re.findall(reg, r.text)  
    return links  
#多页处理，下载到文件  
def get_content(link):  
    r1=requests.get(link,headers,timeout=10)  
    s=requests.session()  
    s.keep_alive = False  
    r1.encoding = 'gbk'  
    t1=html.fromstring(r1.text)  
    try:  
        job=t1.xpath('//div[@class="tHeader tHjob"]//h1/text()')[0]  
        company = t1.xpath('//p[@class="cname"]/a/text()')[0]  
        print(company)  
        label=t1.xpath('//p[@class="t2"]/span/text()')  
        education=t1.xpath('//div[@class="t1"]//span[2]/text()')[0]  
        salary = re.findall(re.compile(r'<span class="lname">.*?<strong>(.*?)</strong>',re.S),r1.text)[0]  
        area = t1.xpath('//div[@class="tHeader tHjob"]//span[@class="lname"]/text()')[0]  
        companytype=t1.xpath('//p[@class="msg ltype"]/text()')  
        workyear=t1.xpath('//div[@class="t1"]//span[1]/text()')[0]  
        describe = re.findall(re.compile(r'<div class="bmsg job_msg inbox">(.*?)任职要求',re.S),r1.text)  
        require =  re.findall(re.compile(r'<div class="bmsg job_msg inbox">.*?任职要求(.*?)<div class="mt10">',re.S),r1.text)  
        try:  
            file = codecs.open('51job.xls', 'a+', 'utf-8')  
            item = str(company)+'\t'+str(job)+'\t'+str(education)+'\t'+str(label)+'\t'+str(salary)+'\t'+str(companytype)+'\t'+str(workyear)+'\t'+str(area)+'\t'+str(workyear)+str(describe)+'\t'+str(require)+'\n'  
            file.write(item)  
            file.close()  
            return True  
        except Exception as e:  
            print(e)  
            return None  
        #output='{},{},{},{},{},{},{},{}\n'.format(company,job,education,label,salary,area,describe,require)  
        #with open('51job.csv', 'a+', encoding='utf-8') as f:  
            #f.write(output)  
    except:  
        print('None')  
  
for  i in range(1,2000):  
    print('正在爬取第{}页信息'.format(i))  
    try:  
        #time.sleep(random.random()+random.randint(1,5))  
        links=get_links(i)  
        for link in links:  
            get_content(link)  
            #time.sleep(random.random()+random.randint(0,1))  
    except:  
        continue  
        print('有点问题')   
 
 ```   
 
 # 三、数据探索和预处理：
 数据爬取完了，大概有十万条数据，看起来还不错。
 ![image](https://raw.githubusercontent.com/lbship/lbship.github.io/master/img/51job1.png)
